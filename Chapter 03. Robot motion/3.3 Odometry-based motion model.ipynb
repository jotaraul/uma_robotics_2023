{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3. Odometry-based motion model\n",
    "\n",
    "**Odometry** can be defined as the sum of wheel encoder pulses (see Fig. 1) to compute the robot pose. In this way, most robot bases/platforms provide some form of *odometry information*, a measurement of how much the robot has moved in reality. It is fun to know that odometry comes from the Greek words ὁδός [odos] (route) and μέτρον [metron] (measurement), which mean *measurement of the route*.\n",
    "\n",
    "<figure style=\"text-align:center\">\n",
    "  <img src=\"images/fig3-3-encoder.png\" width=\"500\" alt=\"\">\n",
    "  <figcaption>Fig. 1: Example of a wheel encoder used to sum pulses and compute the robot pose.</figcaption>\n",
    "</figure>\n",
    "\n",
    "Such information is yielded by the firmware of the robotic base, which computes it at very high rate (*e.g.* at 100Hz) considering constant linear $v_t$ and angular $w_t$ velocities. Concretely, if we know the total number of markers $n_{total}$ (empty holes in the mask)  the encoder has, the angle that the wheel turns per marker can be computed as:\n",
    "\n",
    "$$\\alpha=\\frac{2\\pi}{n_{total}} \\, \\, \\text{(radians)}$$\n",
    "\n",
    "This is detected each time a pulse occurs. Then, in a given time interval $\\Delta t$, the total angle rotated by the wheel given the number of pulses detected $n_t$ is:\n",
    "\n",
    "$$\\Delta \\beta_t=n_t \\cdot \\alpha \\, \\, \\text{(radians)}$$\n",
    "\n",
    "This way, the angular velocity $\\omega$ of the wheel can be computed as:\n",
    "\n",
    "$$\\omega\\simeq \\frac{\\Delta \\beta_t}{\\Delta t} \\, \\, \\text{(radians/seconds)}$$\n",
    "\n",
    "Note that this angular is speed is different from the one w.r.t. the ICR. Since we are considering a differential drive locomotion system, the pose increment can be retrieved as:\n",
    "\n",
    "$$\\Delta p = \\begin{bmatrix} \\Delta x \\\\ \\Delta y \\\\ \\Delta \\theta \\end{bmatrix} = \n",
    "\\begin{bmatrix} \n",
    "\\frac{v_p}{w}sin(w\\Delta t) \\\\ \n",
    "\\frac{v_p}{w}[1-cos(w\\Delta t)] \\\\ \n",
    "w\\Delta t \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "being $w=\\frac{v_r-v_l}{l}$ the angular velocity of the robot w.r.t. the ICR (with $l$ the distance between the wheels), $v_r$ and $v_l$ the linear velocities of the right and left wheels respectly, that can be computed from the previously obtained angular velocities $\\omega_r$ and $\\omega_l$ with  $v=r\\cdot \\omega$ ($r$ stands for the wheel radius), and $v_p$ the linear velocity at the robot-axis midpoint that can be computed as $v_p=\\frac{v_l+v_r}{2}$.\n",
    "\n",
    "As commented, the firmware of the robotic base computes these pose increments at a very high rate, and makes it available to the robot at lower rate (*e.g.* 10Hz) using a tool that we already know: the composition of poses:\n",
    "\n",
    "<figure style=\"text-align:center\">\n",
    "  <img src=\"images/fig3-3-odometry_compositions.PNG\" alt=\"\">\n",
    "  <figcaption>Fig. 2: Example of composition of poses based on odometry.</figcaption>\n",
    "</figure>\n",
    "\n",
    "Note that between the two odometry poses provided by the robotic base, there have been a series of pose increments computed by said firmware. \n",
    "\n",
    "The **odometry motion model** consists of the utilization of such information that, although technically being a measurement rather than a control, will be \n",
    "treated as a control command to simplify the modeling. Thus, the odometry commands take the form of:\n",
    "\n",
    "$$\n",
    "    u_t = f(odom_t,odom_{t-1}) = \\begin{bmatrix}\n",
    "            \\Delta x  \\\\\n",
    "            \\Delta y \\\\\n",
    "            \\Delta \\theta\n",
    "        \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "being $odom_t$ and $odom_{t-1}$ measurements taken as control and computed from the odometry at time instants $t$ and $t-1$.\n",
    "\n",
    "We will implement this motion model in two different forms:\n",
    "- Analytical form, where the motion command is an increment: $u_t=[\\Delta x_t, \\Delta y_t, \\Delta \\theta_t]^T$\n",
    "- Sample form, where it is a combination of a rotation, motion in straight line, and rotation: $u_t=[\\theta_1,d,\\theta_2]^T$\n",
    "\n",
    "In this way, the utilization of the odometry motion model is more suitable to keep track and estimate the robot pose in contrast to the *velocity model*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "# IMPORTS\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from utils.DrawRobot import DrawRobot\n",
    "from utils.PlotEllipse import PlotEllipse\n",
    "from utils.pause import pause\n",
    "from utils.Jacobians import J1, J2\n",
    "from utils.tcomp import tcomp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:green\">OPTIONAL</span>\n",
    "\n",
    "<span style=\"color:green\">Let's compute an odometry pose as the robot base firmware does! Implement a method that, given a number of pulses detected in both wheels, computes the angles that the wheels turned and the resultant angular velocities. Then, implement a second one that retrieves the robot pose increment from those velocities, given a time increment $\\Delta t$. Finally, given a vector of pulses detected from each wheel, compute their respective pose increments, and provide the final odometry pose.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<span style=\"color:green\">***END OF OPTIONAL PART***</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.1 Analytic form\n",
    "\n",
    "Just as we did in chapter 3.1, the analytic form of the odometry motion model uses the composition of poses to model the robot's movement, providing only a notion of how much the pose has changed, not how did it get there. \n",
    "\n",
    "As with the *velocity model*, the odometry one uses a gaussian distribution to represent the **robot pose**, so $x_t \\sim(\\overline{x}_t, \\Sigma_{x_t})$, being its mean and covariance computed as:\n",
    "\n",
    "- **Mean:**\n",
    "$$\\overline{x}_t \n",
    "= \n",
    "g(\\overline{x}_{t-1},\\overline{u}_t)\n",
    "= \n",
    "\\overline{x}_{t-1}\\oplus \\overline{u}_t \\\\[10pt]$$\n",
    "where $u_t=[\\Delta x_t, \\Delta y_t, \\Delta \\theta_t]^T$, so: $\\\\[10pt]$\n",
    "$$\n",
    "    g(\\overline{x}_{t-1},\\overline{u}_t)\n",
    "    = \\begin{bmatrix}\n",
    "        x_1 + \\Delta x \\cos \\theta_1 - \\Delta y \\sin \\theta_1 \\\\ \n",
    "        y_1 + \\Delta x \\sin \\theta_1 - \\Delta y \\cos \\theta_1 \\\\\n",
    "        \\theta_1 + \\Delta \\theta\n",
    "      \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- **Covariance:** \n",
    "$$\\Sigma_{x_t} =  \n",
    "\\frac{\\partial g}{\\partial x_{t-1}} \\cdot \\Sigma_{x_{t-1}} \\cdot {\\frac{\\partial g}{\\partial x_{t-1}}}^T \n",
    "+ \n",
    "\\frac{\\partial g}{\\partial u_{t}} \\cdot \\Sigma_{u_t} \\cdot {\\frac{\\partial g}{\\partial u_{t}}}^T \\\\[10pt]$$\n",
    "where $\\partial g / \\partial x_{t-1}$ and $\\partial g / \\partial u_{t}$ are the jacobians of our motion model evaluated at the previous pose $x_{t-1}$ and the current command $u_t$:\n",
    "$$\n",
    "    \\frac{\\partial g}{\\partial x_{k-1}} \n",
    "    = \n",
    "    \\begin{bmatrix}\n",
    "    1 & 0 & -\\Delta x_k \\sin \\theta_{k-1}-\\Delta y_k \\cos \\theta_{k-1} \\\\\n",
    "    0 & 1 & \\Delta x_k \\cos \\theta_{k-1} - \\Delta y_k \\sin \\theta_{k-1} \\\\\n",
    "    0 & 0 & 1\n",
    "    \\end{bmatrix}\n",
    "    \\, \\, \\, \\,  \\,  \\,  \\,  \\,  \\,  \\,  \\,  \\, \n",
    "    \\frac{\\partial g}{\\partial u_k}\n",
    "    = \n",
    "    \\begin{bmatrix}\n",
    "    \\cos \\theta_{k-1} & -\\sin \\theta_{k-1} & 0  \\\\\n",
    "    \\sin \\theta_{k-1} & \\cos \\theta_{k-1} & 0 \\\\\n",
    "    0 & 0 & 1\n",
    "    \\end{bmatrix} \\\\[10pt]\n",
    "$$\n",
    "and the covariance matrix of this movement ($\\Sigma_{u_t}$) is defined as seen below. Typically, it is constant during robot motion:$\\\\[10pt]$\n",
    "$$ \n",
    "    \\Sigma_{u_t} = \\begin{bmatrix}\n",
    "            \\sigma_{\\Delta x}^2 & 0 & 0\\\\\n",
    "            0 &  \\sigma_{\\Delta y}^2 & 0 \\\\\n",
    "            0 & 0 &  \\sigma_{\\Delta \\theta}^2 \n",
    "        \\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 1: The model in action</i></b></span>**\n",
    "\n",
    "Similarly to the assignment 3.1, we'll move a robot along a 8-by-8 square (in meters), in increments of 2m. In this case you have to complete:\n",
    "\n",
    "- The `step()` method to compute:\n",
    "  - the new expected pose (`self.pose`), \n",
    "  - the new true pose $x_t$ (ground-truth `self.true_pose`) after adding some noise using [`stats.multivariate_normal.rvs()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.multivariate_normal.html) to the movement command $u$ according to `Q` (which represents $\\Sigma_{u_t}$), \n",
    "  - and to update the uncertainty about the robot position in `self.P` (covariance matrix $\\Sigma_{x_t}$). Note that the methods `J1()` and `J2()` already implement  $\\partial g / \\partial x_{t-1}$ and $\\partial g / \\partial u_{t}$ for you, you just have to call them with the right input parameters.\n",
    "- The `draw()` method to plot:\n",
    "  - the uncertainty of the pose as an ellipse centered at the expected pose, and \n",
    "  - the true position (ground-truth).\n",
    "\n",
    "We are going to consider the following motion covariance matrix (it is already coded for you):\n",
    "\n",
    "$$\n",
    "    \\Sigma_{u_t} = \\begin{bmatrix}\n",
    "        0.04 & 0 & 0 \\\\\n",
    "        0 & 0.04 & 0 \\\\\n",
    "        0 & 0 & 0.01\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Example**\n",
    "\n",
    "<figure style=\"text-align:center\">\n",
    "  <img src=\"images/fig3-3-1.png\" width=\"400\" alt=\"\">\n",
    "  <figcaption>Fig. 2: Movement of a robot using odometry commands. <br/> Representing the expected pose (in red), the true pose (as dots) <br/> and the confidence ellipse.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Robot():\n",
    "    \"\"\" Simulation of a robot base\n",
    "    \n",
    "        Attrs:\n",
    "            pose: Expected pose of the robot\n",
    "            P: Covariance of the current pose\n",
    "            true_pose: Real pose of the robot(affected by noise)\n",
    "            Q: Covariance of the movement\n",
    "    \"\"\"\n",
    "    def __init__(self, x, P, Q):\n",
    "        self.pose = x\n",
    "        self.P = P\n",
    "        self.true_pose = self.pose\n",
    "        self.Q = Q\n",
    "        \n",
    "    def step(self, u):\n",
    "        # TODO Update expected pose\n",
    "        prev_pose = self.pose\n",
    "        self.pose = tcomp(None, None)\n",
    "        \n",
    "        # TODO Generate true pose \n",
    "        noisy_u = np.vstack(None)\n",
    "        self.true_pose = tcomp(None, None)\n",
    "        \n",
    "        # TODO Update covariance\n",
    "        JacF_x = J1(None, None)\n",
    "        JacF_u = J2(None, None)\n",
    "\n",
    "        self.P = (\n",
    "            (None @ None @ None) \n",
    "            + (None @ None @ None)\n",
    "        )\n",
    "    \n",
    "    def draw(self, fig, ax):\n",
    "        DrawRobot(fig, ax, self.pose)\n",
    "        el = PlotEllipse(fig, ax, None, None)\n",
    "        ax.plot(None, None, 'o', color=el[0].get_color())\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following demo to **try your new `Robot()` class**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_odometry_commands_analytical(robot):  \n",
    "    # MATPLOTLIB\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim([-3, 11])\n",
    "    ax.set_ylim([-3, 11])\n",
    "    plt.ion()\n",
    "    plt.grid()\n",
    "    plt.fill([2, 2, 6, 6],[2, 6, 6, 2],facecolor='lightgray', edgecolor='gray', linewidth=3)\n",
    "    plt.tight_layout()    \n",
    "    fig.canvas.draw()\n",
    "    \n",
    "    # MOVEMENT PARAMETERS\n",
    "    nSteps = 15\n",
    "    ang = -np.pi/2 # angle to turn in corners\n",
    "    u = np.vstack((2., 0., 0.))\n",
    "    \n",
    "    # MAIN LOOP\n",
    "    for i in range(nSteps):\n",
    "        # change angle on corners\n",
    "        if i % 4 == 3:\n",
    "            u[2, 0] = ang\n",
    "\n",
    "        #Update positions\n",
    "        robot.step(u)\n",
    "\n",
    "        # Restore angle iff changed\n",
    "        if i % 4 == 3:\n",
    "            u[2, 0] = 0\n",
    "\n",
    "        # Draw every loop\n",
    "        robot.draw(fig, ax)\n",
    "        clear_output(wait=True)\n",
    "        display(fig)        \n",
    "        time.sleep(0.3)\n",
    "        \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.vstack([0., 0., np.pi/2]) # pose inicial\n",
    "\n",
    "# Probabilistic parameters\n",
    "P = np.diag([0., 0., 0.])\n",
    "Q = np.diag([0.04, 0.04, 0.01])\n",
    "\n",
    "robot = Robot(x, P, Q)\n",
    "demo_odometry_commands_analytical(robot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\"><b><i>Thinking about it (1)</i></b></font>\n",
    "\n",
    "Once you have completed this assignment regarding the analytical form of the odometry model, **answer the following questions**:\n",
    "\n",
    "- Which is the difference between the $g(\\cdot)$ function used here, and the one in the velocity model?\n",
    "\n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "\n",
    "- How many parameters compound the motion command $u_t$ in this model?\n",
    "\n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "    \n",
    "- Which is the role of the Jacobians $\\partial g / \\partial x_{t-1}$ and $\\partial g / \\partial u_{t}$?\n",
    "\n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "\n",
    "- What happens if you modify the covariance matrix $\\Sigma_{u_t}$ modeling the uncertainty in the motion command $u_t$? Try different values and discuss the results. \n",
    "\n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.2 Sample form\n",
    "\n",
    "The analytical form used above, although useful for the probabilistic algorithms we will cover in this course, does not work well for sampling algorithms such as particle filters.\n",
    "\n",
    "The reason being, if we generate random samples from the gaussian distributions as in the previous exercise, we will find some poses that are not feasible to the non-holonomic movement of a robot, i.e. they do not correspond to a velocity command $(v, w)$ with noise.\n",
    "\n",
    "The following *sample form* is a more realistic way to generate samples of the robot pose. In this case, the movement of the robot is modeled as a sequence of actions (see Fig 3): \n",
    "\n",
    "1. **Turn** ($\\theta_1$): to face the destination point.\n",
    "2. **Advance** ($d$): to arrive at the destination.\n",
    "3. **Turn** ($\\theta_2$): to get to the desired angle.\n",
    "\n",
    "<figure style=\"text-align:center\">\n",
    "  <img src=\"images/fig3-3-odometry_sample_form.png\"  alt=\"\">\n",
    "  <figcaption>Fig. 3: Movement of a robot using odometry commands in sampling form.</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "So this type of order is expressed as:\n",
    "\n",
    "$$\n",
    "    u_t = \\begin{bmatrix}\n",
    "            \\theta_1  \\\\\n",
    "            d \\\\\n",
    "            \\theta_2\n",
    "        \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "It can easily be generated from odometry poses $[\\hat x_t, \\hat y_t,\\hat \\theta_t]^T$ and $[\\hat x_{t-1}, \\hat y_{t-1},\\hat \\theta_{t-1}]^T$ given the following equations:\n",
    "\n",
    "$$\n",
    "    \\begin{equation}\n",
    "    \\theta_1 =atan2(\\hat y_t -\\hat y_{t-1}, \\hat x_t -\\hat x_{t-1})- \\hat \\theta_{t-1} \\\\\n",
    "    d = \\sqrt{(\\hat y_t -\\hat y_{t-1})^2 + (\\hat x_t -\\hat x_{t-1})^2} \\\\\n",
    "    \\theta_2  = \\hat{\\theta}_t - \\hat{\\theta}_{t-1} - \\theta_1\n",
    "    \\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 2: Implementing the sampling form</i></b></span>**\n",
    "\n",
    "Complete the following cells to experience the motion of a robot using the sampling form of the odometry model. For that:\n",
    "\n",
    "1. Implement a function that, given the previously mentioned $[\\hat x_t, \\hat y_t,\\hat \\theta_t]^T$ and $[\\hat x_{t-1}, \\hat y_{t-1},\\hat \\theta_{t-1}]^T$ generates an order $u_t = [ \\theta_1, d , \\theta_2 ]^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_move(pose_now, pose_old):\n",
    "    diff = pose_now - pose_old\n",
    "    theta1 = np.arctan2(None) - None\n",
    "    d = np.sqrt(None)\n",
    "    theta2 = None\n",
    "    return np.vstack((theta1, d, theta2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try such function** with the code cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_move(np.vstack([0., 0., 0.]), np.vstack([1., 1., np.pi/2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Expected output for the commented example:\n",
    "\n",
    "  ```\n",
    "  array([[-3.92699082],\n",
    "       [ 1.41421356],\n",
    "       [ 2.35619449]])\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Using the resulting control action $u_t = [\\hat \\theta_1, \\hat d, \\hat \\theta_2]^T$ we can model its noise in the following way:\n",
    "\n",
    "  $$\n",
    "    \\begin{equation}\n",
    "        \\theta_1 = \\hat\\theta_1 + \\text{sample}\\left(\\alpha_0 \\hat\\theta_1^2 + \\alpha_1 \\hat d^2 \\right) \\\\\n",
    "        d = \\hat d + \\text{sample}\\left(\\alpha_2 \\hat d^2 + \\alpha_3 \\left(\\hat\\theta_1^2 + \\hat d^2 \\right) \\right) \\\\\n",
    "        \\theta_2 = \\hat\\theta_2 + \\text{sample}\\left(\\alpha_0 \\hat\\theta_2^2 + \\alpha_1 \\hat d^2\\right)\n",
    "    \\end{equation}\n",
    "  $$\n",
    "\n",
    "  Where $sample(b)$ generates a random value from a distribution $N(0, b)$. The vector $\\alpha = [\\alpha_0, \\dots, \\alpha_3]$ (`a` in the code), models the robot's intrinsic noise.\n",
    "\n",
    "  The pose of the robot at the end of the movement is computed as follows:\n",
    "$$\n",
    "    \\begin{equation}\n",
    "        x_t = x_{t-1} + d \\cos\\left(\\theta_{t-1} + \\theta_1 \\right) \\\\\n",
    "        y_t = y_{t-1} + d \\sin\\left(\\theta_{t-1} + \\theta_1 \\right) \\\\\n",
    "        \\theta_t = \\theta_{t-1} +  \\theta_1 +  \\theta_2\n",
    "    \\end{equation}\n",
    "$$\n",
    "Complete the `step()` and `draw()` methods to:\n",
    "  - Update the expected robot pose (`self.pose`) and generate new samples. The number of samples is set by `n_samples`, and `self.samples` is in charge of storing such samples. Each sample can be interpreted as one possible pose reached by the robot.\n",
    "  - Draw the true pose of the robot (without angle) as a cloud of particles (samples of possible points which the robot can be at). \n",
    "Play a bit with different values of `a`. To improve this visualization the robot will move in increments of $0.5$ and we are going to plot the particles each 4 increments.\n",
    "\n",
    "**Example**\n",
    "\n",
    "<figure style=\"text-align:center\">\n",
    "  <img src=\"images/fig3-3-2.png\" width=\"400\" alt=\"\">\n",
    "  <figcaption>Fig. 1: Movement of a robot using odometry commands in sampling form. <br/> Representing the expected pose (in red) and the samples (as clouds of dots) </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampledRobot(object):\n",
    "    def __init__(self, mean, a, n_samples):\n",
    "        self.pose = mean\n",
    "        self.a = a\n",
    "        self.samples = np.tile(mean, n_samples)\n",
    "        \n",
    "    def step(self, u):\n",
    "        # TODO Update pose\n",
    "        ang = self.pose[2, 0] + u[0, 0]\n",
    "        self.pose[0, 0] += None\n",
    "        self.pose[1, 0] += None\n",
    "        self.pose[2, 0] = None\n",
    "                \n",
    "        # TODO Generate new samples\n",
    "        sample = lambda b: stats.norm(loc=0, scale=b).rvs(size=self.samples.shape[1])\n",
    "        \n",
    "        u2 = u**2\n",
    "        \n",
    "        noisy_u = u + np.vstack((\n",
    "            sample(None),\n",
    "            sample(None),\n",
    "            sample(None)\n",
    "        ))\n",
    "        \n",
    "        # TODO Update particles (robots) poses\n",
    "        ang = self.samples[2, :] + noisy_u[0, :]\n",
    "        \n",
    "        self.samples[0, :] += None\n",
    "        self.samples[1, :] += None\n",
    "        self.samples[2, :] = None\n",
    "        \n",
    "    def draw(self, fig, ax):\n",
    "        DrawRobot(fig, ax, self.pose)\n",
    "        ax.plot(None[0, :], None[1, :], '.')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following demo to **test your code**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_odometry_commands_sample(robot):\n",
    "    # PARAMETERS\n",
    "    inc = .5\n",
    "    show_each = 4\n",
    "    limit_iterations = 32\n",
    "    \n",
    "    # MATPLOTLIB\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim([-3, 11])\n",
    "    ax.set_ylim([-3, 11])\n",
    "    plt.ion()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # MAIN LOOP\n",
    "    robot.draw(fig, ax)\n",
    "    inc_pose = np.vstack((0., inc, 0.))\n",
    "    \n",
    "    for i in range(limit_iterations):\n",
    "        if i == 16:\n",
    "            inc_pose[0, 0] = inc\n",
    "            inc_pose[1, 0] = 0\n",
    "            inc_pose[2, 0] = -np.pi/2\n",
    "            \n",
    "        u = generate_move(robot.pose+inc_pose, robot.pose)\n",
    "        \n",
    "        robot.step(u)\n",
    "        \n",
    "        if i == 16:\n",
    "            inc_pose[2, 0] = 0\n",
    " \n",
    "        if i % show_each == show_each-1:\n",
    "            robot.draw(fig, ax)\n",
    "            clear_output(wait=True)\n",
    "            display(fig)        \n",
    "            time.sleep(0.1)\n",
    "            \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RUN \n",
    "n_particles = 100\n",
    "a = np.array([.07, .07, .03, .05])\n",
    "x = np.vstack((0., 0., np.pi/2))\n",
    "\n",
    "robot = SampledRobot(x, a, n_particles)\n",
    "demo_odometry_commands_sample(robot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\"><b><i>Thinking about it (2)</i></b></font>\n",
    "\n",
    "Now you are an expert in the sample form of the odometry motion model! **Answer the following questions**:\n",
    "\n",
    "- Which is the effect of modifying the robot's intrinsic noise $\\alpha$ (`a` in the code)?\n",
    "\n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "\n",
    "- How many parameters compound the motion command $u_t$ in this model?\n",
    "\n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "    \n",
    "- After moving the robot a sufficient number of times, what shape does the distribution of samples take?\n",
    "\n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
